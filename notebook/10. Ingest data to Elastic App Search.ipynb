{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import analyzer, Document, Date, Text, Integer, Keyword, Double\n",
    "from elastic_enterprise_search import AppSearch\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Elastic App Search\n",
    "app_search = AppSearch(\n",
    "    \"http://localhost:3002\",\n",
    "    http_auth=\"private-6jj3ai4ckkq2xykcocosmv6o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a document class using data types from elasticsearch_dsl\n",
    "class Listing(Document):\n",
    "    id = Text()\n",
    "    org_id = Integer()\n",
    "    listing_url = Text()\n",
    "    scrape_id = Integer()\n",
    "    last_scraped = Keyword()\n",
    "    crawled_date = Date()\n",
    "    name = Text(analyzer='snowball')\n",
    "    host_id = Integer()\n",
    "    host_is_superhost = Keyword()\n",
    "    host_identity_verified = Text(fields={'raw': Keyword()})\n",
    "    room_type = Text(fields={'raw': Keyword()})\n",
    "    accommodates = Integer()\n",
    "    guests_included = Integer()\n",
    "    minimum_nights = Integer()\n",
    "    maximum_nights = Integer()\n",
    "    calendar_updated = Text(fields={'raw': Keyword()})\n",
    "    instant_bookable = Keyword()\n",
    "    is_business_travel_ready = Keyword()\n",
    "    cancellation_policy = Text(fields={'raw': Keyword()})\n",
    "    price = Integer()\n",
    "    availability_30 = Integer()\n",
    "    availability_60 = Integer()\n",
    "    availability_90 = Integer()\n",
    "    availability_365 = Integer()\n",
    "    number_of_reviews = Integer()\n",
    "    first_review = Text(fields={'raw': Keyword()})\n",
    "    last_review = Text(fields={'raw': Keyword()})\n",
    "    review_scores_rating = Integer()\n",
    "    review_scores_accuracy = Integer()\n",
    "    review_scores_cleanliness = Integer()\n",
    "    review_scores_checkin = Integer()\n",
    "    review_scores_communication = Integer()\n",
    "    review_scores_location = Integer()\n",
    "    review_scores_value = Integer()\n",
    "    overall_rating = Double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for processing documents\n",
    "def clean_currency(price):\n",
    "    \n",
    "    if '$' in price:\n",
    "        price = price.replace('$', '')\n",
    "\n",
    "    if ',' in price:\n",
    "        price = price.replace(',', '')\n",
    "\n",
    "    return price\n",
    "\n",
    "def get_overall_rating(row):\n",
    "    \"\"\" Get overall rating using review fields as target indicator \"\"\"\n",
    "    \n",
    "    review_scores_accuracy = float(row['review_scores_accuracy'])\n",
    "    review_scores_cleanliness = float(row['review_scores_cleanliness'])\n",
    "    review_scores_checkin = float(row['review_scores_checkin'])\n",
    "    review_scores_communication = float(row['review_scores_communication'])\n",
    "    review_scores_location = float(row['review_scores_location'])\n",
    "    review_scores_value = float(row['review_scores_value'])\n",
    "\n",
    "    overall_rating = (((review_scores_accuracy + review_scores_cleanliness \\\n",
    "                      + review_scores_checkin + review_scores_communication \\\n",
    "                      + review_scores_location + review_scores_value) / 2.0) / 6.0)\n",
    "    return overall_rating\n",
    "\n",
    "def validate_price(df):\n",
    "    \"\"\" Validate price (if exists), otherwise assign default a value\"\"\"\n",
    "    \n",
    "    # Convert 'price' to float\n",
    "    if('price' in df.columns):\n",
    "        \n",
    "        # Fill rows with null 'price'\n",
    "        df['price'].fillna(value='0', inplace=True)\n",
    "\n",
    "        df['price'] = df['price'].apply(clean_currency).astype('float')\n",
    "\n",
    "    # Handle data with no 'price', e.g., athens_2020-07-21_data_listings.csv.gz\n",
    "    elif ('price' not in df.columns):\n",
    "        \n",
    "        if ('weekly_price' in df.columns):\n",
    "\n",
    "            # Fill rows with null 'weekly_price'\n",
    "            df['weekly_price'].fillna(value='0', inplace=True)\n",
    "            \n",
    "            df['weekly_price'] = df['weekly_price'].apply(clean_currency).astype('float')\n",
    "            df['price'] = df['weekly_price'] / 7.0\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Set missigng 'price' and 'weekly_price' to 0\n",
    "            df['price'].fillna(value='0', inplace=True)\n",
    "            df['weekly_price'].fillna(value='0', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_crawled_date(df):\n",
    "    \"\"\" Extract crawled date from the 'scrape_id' field. \"\"\"\n",
    "\n",
    "    df['crawled_date'] = df['scrape_id'].astype(str)\n",
    "    df['crawled_date'] = df['crawled_date'].apply(lambda x: x[:8])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gen_missing_columns(df):\n",
    "    \"\"\" Extract 'guests_included' from the 'accommodates' field. \"\"\"\n",
    "\n",
    "    if 'host_is_superhost' not in df.columns:\n",
    "        df['host_is_superhost'] = \"f\"\n",
    "\n",
    "    if 'host_identity_verified' not in df.columns:\n",
    "        df['host_identity_verified'] = \"f\"\n",
    "\n",
    "    if 'room_type' not in df.columns:\n",
    "        df['room_type'] = \"n/a\"\n",
    "\n",
    "    if 'accommodates' not in df.columns:\n",
    "        df['accommodates'] = 0\n",
    "\n",
    "    if 'guests_included' not in df.columns:\n",
    "        df['guests_included'] = df['accommodates']\n",
    "\n",
    "    if 'minimum_nights' not in df.columns:\n",
    "        df['minimum_nights'] = 0\n",
    "\n",
    "    if 'maximum_nights' not in df.columns:\n",
    "        df['maximum_nights'] = 0\n",
    "\n",
    "    if 'calendar_updated' not in df.columns:\n",
    "        df['calendar_updated'] = \"n/a\"\n",
    "\n",
    "    if 'instant_bookable' not in df.columns:\n",
    "        df['instant_bookable'] = \"f\"\n",
    "\n",
    "    if 'is_business_travel_ready' not in df.columns:\n",
    "        df['is_business_travel_ready'] = \"f\"\n",
    "\n",
    "    if 'cancellation_policy' not in df.columns:\n",
    "        df['cancellation_policy'] = \"n/a\"\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_features(df):\n",
    "    \"\"\" Select specific columns and convert date columnd to string. \"\"\"\n",
    "    \n",
    "    df = df[\n",
    "            [ \n",
    "                'id', 'listing_url', 'scrape_id', 'last_scraped', 'crawled_date', \n",
    "                'name', 'host_id', 'host_is_superhost', 'host_identity_verified', \n",
    "                'room_type', 'accommodates', 'guests_included','minimum_nights', \n",
    "                'maximum_nights', 'calendar_updated', 'instant_bookable', 'is_business_travel_ready', 'cancellation_policy',\n",
    "                'price', 'availability_30', 'availability_60', 'availability_90', 'availability_365', \n",
    "                'number_of_reviews', 'first_review', 'last_review', 'review_scores_rating', \n",
    "                'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', \n",
    "                'review_scores_communication', 'review_scores_location', 'review_scores_value'\n",
    "            ]\n",
    "    ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_reviews(df):\n",
    "    \"\"\" Enrich no review records with default review scores. \"\"\"\n",
    "    \n",
    "    df['first_review'].fillna(value='1991-01-01', inplace=True)\n",
    "    df['last_review'].fillna(value='0', inplace=True)\n",
    "    df['review_scores_rating'].fillna(value=0, inplace=True)\n",
    "    df['review_scores_accuracy'].fillna(value=0, inplace=True)\n",
    "    df['review_scores_accuracy'].fillna(value=0, inplace=True)\n",
    "    df['review_scores_cleanliness'].fillna(value=0, inplace=True)\n",
    "    df['review_scores_checkin'].fillna(value=0, inplace=True)\n",
    "    df['review_scores_communication'].fillna(value=0, inplace=True)\n",
    "    df['review_scores_location'].fillna(value=0, inplace=True)\n",
    "    df['review_scores_value'].fillna(value=0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_null_values(df):\n",
    "    \"\"\" Drop records with NaN values. \"\"\"\n",
    "    \n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "def fill_null_values(df):\n",
    "    \"\"\" Fill records with NaN values. \"\"\"\n",
    "    \n",
    "    df['listing_url'].fillna(value=' ', inplace=True)\n",
    "    df['scrape_id'].fillna(value=0, inplace=True)\n",
    "    df['last_scraped'].fillna(value='1991-01-01', inplace=True)\n",
    "    df['crawled_date'].fillna(value='1991-01-01', inplace=True)\n",
    "    df['name'].fillna(value=' ', inplace=True)\n",
    "    df['host_id'].fillna(value=0, inplace=True)\n",
    "    df['host_is_superhost'].fillna(value=' ', inplace=True)\n",
    "    df['host_identity_verified'].fillna(value=' ', inplace=True)\n",
    "    df['room_type'].fillna(value=' ', inplace=True)\n",
    "    df['accommodates'].fillna(value=0, inplace=True)\n",
    "    df['guests_included'].fillna(value=0, inplace=True)\n",
    "    df['minimum_nights'].fillna(value=0, inplace=True)\n",
    "    df['maximum_nights'].fillna(value=0, inplace=True)\n",
    "    df['calendar_updated'].fillna(value=' ', inplace=True)\n",
    "    df['instant_bookable'].fillna(value=' ', inplace=True)\n",
    "    df['is_business_travel_ready'].fillna(value=' ', inplace=True)\n",
    "    df['cancellation_policy'].fillna(value=' ', inplace=True)\n",
    "    df['price'].fillna(value=0, inplace=True)\n",
    "    df['availability_30'].fillna(value=0, inplace=True)\n",
    "    df['availability_60'].fillna(value=0, inplace=True)\n",
    "    df['availability_90'].fillna(value=0, inplace=True)\n",
    "    df['availability_365'].fillna(value=0, inplace=True)\n",
    "    df['number_of_reviews'].fillna(value=0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for indexing documents\n",
    "def ingest_data(df, index, total_docs):\n",
    "    \"\"\" Finalize data and ingest a bulk of documents to ES index \"\"\"\n",
    "\n",
    "    try:\n",
    "        i = 0\n",
    "        bulk_size = 100\n",
    "        docs = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            \n",
    "            # Count the number of documents\n",
    "            i += 1\n",
    "            \n",
    "            doc = Listing()\n",
    "            \n",
    "            if ('id' in row) and ('crawled_date' in row):\n",
    "                org_id = row['id']\n",
    "                crawled_date = row['crawled_date']\n",
    "                \n",
    "                # Generate an unique ID by concating the orignal ID with crawled date\n",
    "                doc.id = str(org_id) + \"-\" + crawled_date\n",
    "                \n",
    "                doc.org_id = org_id\n",
    "                doc.crawled_date = crawled_date\n",
    "            if 'listing_url' in row:\n",
    "                doc.listing_url = row['listing_url']\n",
    "            if 'scrape_id' in row:\n",
    "                doc.scrape_id = row['scrape_id']\n",
    "            if 'last_scraped' in row:\n",
    "                doc.last_scraped = str(row['last_scraped']).replace(\"-\", \"\")\n",
    "            if 'name' in row:\n",
    "                doc.name = row['name']\n",
    "            if 'host_id' in row:\n",
    "                doc.host_id = row['host_id']\n",
    "            if 'host_is_superhost' in row:\n",
    "                doc.host_is_superhost = row['host_is_superhost']\n",
    "            if 'host_identity_verified'in row:\n",
    "                doc.host_identity_verified = row['host_identity_verified']\n",
    "            if 'room_type' in row:\n",
    "                doc.room_type = row['room_type']\n",
    "            if 'accommodates' in row:\n",
    "                doc.accommodates = row['accommodates']\n",
    "            if 'guests_included' in row:\n",
    "                doc.guests_included = row['guests_included']\n",
    "            if 'minimum_nights' in row:\n",
    "                doc.minimum_nights = row['minimum_nights']\n",
    "            if 'maximum_nights' in row:\n",
    "                doc.maximum_nights = row['maximum_nights']\n",
    "            if 'calendar_updated' in row:\n",
    "                doc.calendar_updated = row['calendar_updated']\n",
    "            if 'instant_bookable' in row:\n",
    "                doc.instant_bookable = row['instant_bookable']\n",
    "            if 'is_business_travel_ready' in row:\n",
    "                doc.is_business_travel_ready = row['is_business_travel_ready']\n",
    "            if 'cancellation_policy' in row:\n",
    "                doc.cancellation_policy = row['cancellation_policy']\n",
    "            if 'price' in row:\n",
    "                doc.price = row['price']\n",
    "            if 'availability_30' in row:\n",
    "                doc.availability_30 = row['availability_30']\n",
    "            if 'availability_60' in row:\n",
    "                doc.availability_60 = row['availability_60']\n",
    "            if 'availability_90' in row:\n",
    "                doc.availability_90 = row['availability_90']\n",
    "            if 'availability_365' in row:\n",
    "                doc.availability_365 = row['availability_365']\n",
    "            if 'number_of_reviews' in row:\n",
    "                doc.number_of_reviews = row['number_of_reviews']\n",
    "            if 'first_review' in row:\n",
    "                doc.first_review = str(row['first_review']).replace(\"-\", \"\")\n",
    "            if 'last_review' in row:\n",
    "                doc.last_review = str(row['last_review']).replace(\"-\", \"\")\n",
    "            if 'review_scores_rating' in row:\n",
    "                doc.review_scores_rating = row['review_scores_rating']\n",
    "            if 'review_scores_accuracy' in row:\n",
    "                doc.review_scores_accuracy = row['review_scores_accuracy']\n",
    "            if 'review_scores_cleanliness' in row:\n",
    "                doc.review_scores_cleanliness = row['review_scores_cleanliness']\n",
    "            if 'review_scores_checkin' in row:\n",
    "                doc.review_scores_checkin = row['review_scores_checkin']\n",
    "            if 'review_scores_communication' in row:\n",
    "                doc.review_scores_communication = row['review_scores_communication']\n",
    "            if 'review_scores_location' in row:\n",
    "                doc.review_scores_location = row['review_scores_location']\n",
    "            if 'review_scores_value' in row:\n",
    "                doc.review_scores_value = row['review_scores_value']\n",
    "          \n",
    "            # Compute overall_rating by averaging all reviews scores\n",
    "            overall_rating = get_overall_rating(row)\n",
    "            doc.overall_rating = overall_rating\n",
    "\n",
    "            # Append the current document to a list of documents\n",
    "            docs.append(doc.to_dict(include_meta=False))\n",
    "            \n",
    "            # Ingest a bulk of documents into the current index\n",
    "            if ((i % bulk_size) == 0) or (i == total_docs):\n",
    "                app_search.index_documents(\n",
    "                    engine_name=index,\n",
    "                    documents=docs\n",
    "                )\n",
    "                \n",
    "                # Reset the list of documents\n",
    "                docs = []\n",
    "            \n",
    "    except Exception:\n",
    "        logging.error('exception occured', exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start indexing ...\n",
      "\tCreating a new index with 18033 documents loaded from file: barcelona_2019-01-14_data_listings.csv.gz\n",
      "\tUpdating an existing index with 17763 documents loaded from file: barcelona_2019-02-06_data_listings.csv.gz\n",
      "\tUpdating an existing index with 17807 documents loaded from file: barcelona_2019-03-08_data_listings.csv.gz\n",
      "\tUpdating an existing index with 17899 documents loaded from file: barcelona_2019-04-10_data_listings.csv.gz\n",
      "\tUpdating an existing index with 18302 documents loaded from file: barcelona_2019-05-14_data_listings.csv.gz\n",
      "\tUpdating an existing index with 18837 documents loaded from file: barcelona_2019-06-07_data_listings.csv.gz\n",
      "\tUpdating an existing index with 19833 documents loaded from file: barcelona_2019-07-10_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20556 documents loaded from file: barcelona_2019-08-12_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20404 documents loaded from file: barcelona_2019-09-17_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20147 documents loaded from file: barcelona_2019-10-16_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20428 documents loaded from file: barcelona_2019-11-09_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20843 documents loaded from file: barcelona_2019-12-10_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20708 documents loaded from file: barcelona_2020-01-10_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20981 documents loaded from file: barcelona_2020-02-16_data_listings.csv.gz\n",
      "\tUpdating an existing index with 21116 documents loaded from file: barcelona_2020-03-16_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20838 documents loaded from file: barcelona_2020-04-16_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20858 documents loaded from file: barcelona_2020-05-11_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20864 documents loaded from file: barcelona_2020-06-13_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20517 documents loaded from file: barcelona_2020-07-17_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20703 documents loaded from file: barcelona_2020-08-24_data_listings.csv.gz\n",
      "\tUpdating an existing index with 20337 documents loaded from file: barcelona_2020-09-12_data_listings.csv.gz\n",
      "\tUpdating an existing index with 19896 documents loaded from file: barcelona_2020-10-12_data_listings.csv.gz\n",
      "\tUpdating an existing index with 19896 documents loaded from file: barcelona_2020-11-06_data_listings.csv.gz\n",
      "\tUpdating an existing index with 19641 documents loaded from file: barcelona_2020-12-16_data_listings.csv.gz\n",
      "Finished indexing ...\n",
      "<class 'dict'>\n",
      "{'airbnb-history-barcelona': {'2019-01-14': 18033, '2019-02-06': 17763, '2019-03-08': 17807, '2019-04-10': 17899, '2019-05-14': 18302, '2019-06-07': 18837, '2019-07-10': 19833, '2019-08-12': 20556, '2019-09-17': 20404, '2019-10-16': 20147, '2019-11-09': 20428, '2019-12-10': 20843, '2020-01-10': 20708, '2020-02-16': 20981, '2020-03-16': 21116, '2020-04-16': 20838, '2020-05-11': 20858, '2020-06-13': 20864, '2020-07-17': 20517, '2020-08-24': 20703, '2020-09-12': 20337, '2020-10-12': 19896, '2020-11-06': 19896, '2020-12-16': 19641}}\n"
     ]
    }
   ],
   "source": [
    "# Index listing documents crawled between 2019 - 2020\n",
    "try:\n",
    "    unique_list = []\n",
    "    doc_dist = {}\n",
    "    \n",
    "\n",
    "    print(\"Start indexing ...\")\n",
    "    path = '/Users/nattiya/Desktop/WayBack_InsideAirBNB/'\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        \n",
    "        # Top 10 cities by active listings (https://www.alltherooms.com/analytics/airbnb-statistics/):\n",
    "        #if (file.startswith(\"london\") or file.startswith(\"paris\") or file.startswith(\"new-york-city\") or file.startswith(\"rome\") or file.startswith(\"rio-de-janeiro\") or file.startswith(\"buenos-aires\") or file.startswith(\"sydney\") or file.startswith(\"mexico-city\") or file.startswith(\"barcelona\")) and ((\"2019-\" in file) or (\"2020-\" in file)) and file.endswith(\".csv.gz\"):\n",
    "        if (file.startswith(\"barcelona\")) and ((\"2019-\" in file) or (\"2020-\" in file)) and file.endswith(\".csv.gz\"):\n",
    "            \n",
    "            # Extract month from file\n",
    "            name = file.find(\"_\")\n",
    "            city = file[0:name].lower()\n",
    "            \n",
    "            # Extract city name from file\n",
    "            year = file.find(\"20\")\n",
    "            month = file[year:year+10]\n",
    "\n",
    "            # Load original listing data\n",
    "            df = pd.read_csv(path + file, compression='gzip')\n",
    "\n",
    "            # Pre-process raw data\n",
    "            # Step 1: Enrich raw data with price and crawled date\n",
    "            df = validate_price(df)\n",
    "            df = get_crawled_date(df)\n",
    "            df = gen_missing_columns(df)\n",
    "            raw_count = len(df)\n",
    "\n",
    "            # Step 2: Assign ratings to listings with no reviews\n",
    "            df = get_features(df)\n",
    "            df = validate_reviews(df)\n",
    "            review_count = len(df)\n",
    "\n",
    "            # Step 3: Drop records with null values\n",
    "            #df = drop_null_values(df)\n",
    "            df = fill_null_values(df)\n",
    "            final_count = len(df)\n",
    "\n",
    "            # Obtain the index name\n",
    "            index_name = 'airbnb-history-' + city\n",
    "\n",
    "            # Check if the city is seen for the first time \n",
    "            if index_name not in unique_list:\n",
    "\n",
    "                print(\"\\tCreating a new index with %d documents loaded from file: %s\" % (final_count, file))\n",
    "\n",
    "                unique_list.append(index_name)\n",
    "                \n",
    "                snapshots = {}\n",
    "                snapshots[month] = final_count\n",
    "                doc_dist[index_name] = snapshots\n",
    "\n",
    "                # Initialize index (only perform once)\n",
    "                #resp = app_search.create_engine(\n",
    "                #    engine_name=index_name,\n",
    "                #    language=\"en\"\n",
    "                #)\n",
    "                \n",
    "                # Index documents loaded from the current snapshot\n",
    "                #ingest_data(df, index=index_name, total_docs=final_count)\n",
    "                \n",
    "                # Updating schema\n",
    "                #resp = app_search.put_schema(\n",
    "                #    engine_name=index_name,\n",
    "                #    schema={\n",
    "                #      \"accommodates\": \"number\",\n",
    "                #      \"availability_30\": \"number\",\n",
    "                #      \"availability_365\": \"number\",\n",
    "                #      \"availability_60\": \"number\",\n",
    "                #      \"availability_90\": \"number\",\n",
    "                #      \"guests_included\": \"number\",\n",
    "                #      \"maximum_nights\": \"number\",\n",
    "                #      \"minimum_nights\": \"number\",\n",
    "                #      \"number_of_reviews\": \"number\",\n",
    "                #      \"overall_rating\": \"number\",\n",
    "                #      \"price\": \"number\",\n",
    "                #      \"review_scores_accuracy\": \"number\",\n",
    "                #      \"review_scores_checkin\": \"number\",\n",
    "                #      \"review_scores_cleanliness\": \"number\",\n",
    "                #      \"review_scores_communication\": \"number\",\n",
    "                #      \"review_scores_location\": \"number\",\n",
    "                #      \"review_scores_rating\": \"number\",\n",
    "                #      \"review_scores_value\": \"number\",\n",
    "                #      \"calendar_updated\": \"text\",\n",
    "                #      \"cancellation_policy\": \"text\",\n",
    "                #      \"crawled_date\": \"text\",\n",
    "                #      \"first_review\": \"text\",\n",
    "                #      \"host_id\": \"text\",\n",
    "                #      \"host_identity_verified\": \"text\",\n",
    "                #      \"host_is_superhost\": \"text\",\n",
    "                #      \"instant_bookable\": \"text\",\n",
    "                #      \"is_business_travel_ready\": \"text\",\n",
    "                #      \"last_review\": \"text\",\n",
    "                #      \"last_scraped\": \"text\",\n",
    "                #      \"listing_url\": \"text\",\n",
    "                #      \"name\": \"text\",\n",
    "                #      \"room_type\": \"text\",\n",
    "                #      \"scrape_id\": \"text\"\n",
    "                #    }\n",
    "                #)\n",
    "\n",
    "            else:\n",
    "                print(\"\\tUpdating an existing index with %d documents loaded from file: %s\" % (final_count, file))\n",
    "                \n",
    "                snapshots = doc_dist[index_name]\n",
    "                snapshots[month] = final_count\n",
    "                doc_dist[index_name] = snapshots\n",
    "                \n",
    "                #ingest_data(df, index=index_name, total_docs=final_count)\n",
    "                \n",
    "    print(\"Finished indexing ...\")\n",
    "    print(type(doc_dist))\n",
    "    print(doc_dist)\n",
    "\n",
    "except Exception:\n",
    "    logging.error('exception occured', exc_info=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
